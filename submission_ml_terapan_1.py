# -*- coding: utf-8 -*-
"""Submission ML Terapan 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tGPIM1isFmw0AFdHtkA4Zfi0olQ49L4A

# **Prediksi Pendapatan Berdasarkan Faktor Sosial-Ekonomi**

Sumber Dataset: https://www.kaggle.com/datasets/aldol07/socioeconomic-factors-and-income-dataset/data

## Import Library

Pada tahap awal, dilakukan impor pustaka (library) *Python* yang dibutuhkan untuk keseluruhan proses pengembangan, mulai dari analisis data, visualisasi, hingga pemodelan *machine learning*.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
# %matplotlib inline

from google.colab import files, drive
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV

"""## Data Loading

Tahap ini dilakukan pengumpulan dataset dari sumbernya, lalu dimuat ke notebook dalam bentuk DataFrame untuk bisa melanjutkan ke tahap selanjutnya. Terakhir, dapat dilakukan pengecekan beberapa baris awal untuk memastikan bahwa data telah berhasil dimuat.
"""

!wget https://www.kaggle.com/api/v1/datasets/download/aldol07/socioeconomic-factors-and-income-dataset

!unzip socioeconomic-factors-and-income-dataset

df = pd.read_csv('sgdata.csv')
df.head()

"""## Exploratory Data Analysis

Tahap ini terdiri dari proses-proses analisis untuk memahami karakteristik dataset. Tujuan dari EDA secara rinci dapat dideskripsikan sebagai berikut:

1. Memahami Struktur Data: meninjau jumlah baris dan kolom, serta jenis data dari dataset.
2. Menangani Data yang Hilang dan Outliers: mengidentifikasi missing values dan outliers, lalu menerapkan solusi yang tepat untuk menanganinya.
3. Analisis Distribusi, Korelasi, dan Visualisasi Data: menganalisis distribusi variabel numerik dengan statistik deskriptif dan visualisasi seperti histogram atau boxplot, selain itu juga memeriksa hubungan antara variabel menggunakan matriks korelasi atau scatter plot.

### Memahami Struktur Data
"""

df.info()

df.shape

df.describe()

"""### Identifikasi Nilai Kosong atau Duplikat"""

df.isna().sum()

df.duplicated().sum()

"""Sampai tahap ini, dapat diketahui bahwa **tidak ada** duplikat atau missing values pada dataset.

### Identifikasi Outliers
"""

numerical_features = df.select_dtypes(include='number')
numerical_features.columns

categorical_features = df.select_dtypes(include='object')
categorical_features.columns

def identify_outliers(data):
    Q1 = data[data.columns].quantile(0.25)
    Q3 = data[data.columns].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers_mask = (data[data.columns] < lower_bound) | (data[data.columns] > upper_bound)
    outliers = data[outliers_mask.any(axis=1)].index
    return outliers

identify_outliers(numerical_features)

plt.figure(figsize=(10, 20))
for i, col in enumerate(numerical_features.columns, 1):
    plt.subplot(11, 4, i)
    sns.boxplot(data=numerical_features[col])
    plt.title(f'Boxplot of {col}')
    plt.tight_layout()

"""Pada tahap analisis outliers, diketahui terdapat outliers pada Income dan Age yang jumlahnya mencapai 130. Jika dibandingkan dengan jumlah keseluruhan data, yaitu 2000, maka tidak masalah jika seluruh outliers dibuang dari dataset."""

Q1 = numerical_features.quantile(0.25)
Q3 = numerical_features.quantile(0.75)
IQR = Q3 - Q1

filter_outliers = ~((numerical_features < (Q1 - 1.5 * IQR)) |
                    (numerical_features > (Q3 + 1.5 * IQR))).any(axis=1)

df_filtered = df[filter_outliers]
df_filtered.shape

"""### Analisis Distribusi dan Korelasi"""

plt.figure(figsize=(10, 5))
for i, col in enumerate(numerical_features.columns, 1):
    plt.subplot(2, 3, i)
    sns.histplot(df_filtered[col], bins=30, kde=True)
    plt.title(f'Distribusi {col}')
    plt.tight_layout()

plt.figure(figsize=(12, 10))
for i, col in enumerate(categorical_features.columns, 1):
    plt.subplot(3, 1, i)
    sns.histplot(df_filtered[col], bins=30, kde=True)
    plt.title(f'Distribusi {col}')
    plt.tight_layout()

"""### Univariate Analysis

#### Marital Status
"""

feature = categorical_features.columns[0]
count = df_filtered[feature].value_counts()
percent = 100 * df_filtered[feature].value_counts(normalize=True)

df_preview = pd.DataFrame({'Count': count, 'Percent': percent.round(2)})
print(df_preview)

count.plot(kind='bar', title=feature)
plt.xticks(rotation=15)
plt.show()

"""#### Education"""

feature = categorical_features.columns[1]
count = df_filtered[feature].value_counts()
percent = 100 * df_filtered[feature].value_counts(normalize=True)

df_preview = pd.DataFrame({'Count': count, 'Percent': percent.round(2)})
print(df_preview)

count.plot(kind='bar', title=feature)
plt.xticks(rotation=15)
plt.show()

"""#### Occupation"""

feature = categorical_features.columns[2]
count = df_filtered[feature].value_counts()
percent = 100 * df_filtered[feature].value_counts(normalize=True)

df_preview = pd.DataFrame({'Count': count, 'Percent': percent.round(2)})
print(df_preview)

count.plot(kind='bar', title=feature)
plt.xticks(rotation=15)
plt.show()

"""### Multivariate Analysis"""

numerical_columns = numerical_features.columns.to_list()
categorical_columns = categorical_features.columns.to_list()

for column in categorical_columns:
  sns.catplot(x=column, y='Income', kind='bar', dodge=False, height=4, aspect=3, data=df_filtered)
  plt.title(f'Relativitas Income terhadap {column}')

"""Setelah mengamati relativitas dari rata-rata Income terhadap fitur kategori di atas, didapatkan insight sebagai berikut:
1. Fitur Marital: tidak ada perbedaan signifikan dari Income antara status Single dan Non-single.
2. Fitur Education: status Graduate School memiliki Income paling tinggi dibanding status lainnya.
3. Fitur Occupation: status pekerjaan Management/Self-employed/Highly Qualified/Officer memiliki Income paling tinggi dibanding status lainnya.
"""

sns.pairplot(df_filtered, diag_kind='kde')

plt.figure(figsize=(10, 8))
correlation_matrix = numerical_features.corr().round(2)
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix untuk Fitur Numerik')

"""Fitur ID memiliki korelasi yang sangat kecil (-0.38) dengan fitur Income, sehingga dapat dihilangkan.

## Data Preparation

Pada tahap ini dilakukan transformasi data, guna memastikan kualitas data sebelum digunakan dalam model Machine Learning. Berikut beberapa hal yang dilakukan:
1. Menghapus Kolom ID
2. Data Splitting
4. Feature Encoding & Scaling

### Menghapus Kolom ID
"""

df_filtered = df_filtered.drop(['ID'], axis=1)
df_filtered.head()

"""### Data Splitting"""

X = df_filtered.drop(['Income'], axis=1)
y = df_filtered['Income']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123)

print('Dataset count: ', len(X))
print('Train count: ', len(X_train))
print('Test count: ', len(X_test))

"""### Feature Encoding & Scaling"""

preprocessor = ColumnTransformer([
    ('numerical', StandardScaler(), numerical_features.drop(['ID', 'Income'], axis=1).columns.to_list()),
    ('categorical', OneHotEncoder(handle_unknown='ignore'), categorical_columns),
])

X_train_encoded = preprocessor.fit_transform(X_train)
X_test_encoded = preprocessor.transform(X_test)

column_names = preprocessor.get_feature_names_out()
print(column_names)

"""## Model Development

Setelah data berhasil dibersihkan sebelumnya, selanjutnya dapat menggunakan algoritma Machine Learning untuk menjawab Problem Statement dari tahap Business Understanding. Berikut algoritma yang akan digunakan:
1. K-Neighbors Regressor
2. Random Forest Regressor
3. AdaBoost Regressor
"""

models = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['K-Nearest', 'RandomForest', 'AdaBoost'])

knn = KNeighborsRegressor(n_neighbors=5).fit(X_train_encoded, y_train)
models.loc['train_mse', 'K-Nearest'] = mean_squared_error(y_pred=knn.predict(X_test_encoded), y_true=y_test)

rf = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1).fit(X_train_encoded, y_train)
models.loc['train_mse', 'RandomForest'] = mean_squared_error(y_pred=rf.predict(X_test_encoded), y_true=y_test)

ab = AdaBoostRegressor(learning_rate=0.05, random_state=55).fit(X_train_encoded, y_train)
models.loc['train_mse', 'AdaBoost'] = mean_squared_error(y_pred=ab.predict(X_test_encoded), y_true=y_test)

models

"""## Evaluasi Model

Tahap ini mengukur kinerja masing-masing model yang sudah dikembangkan, sehingga bisa diketahui model mana yang memiliki performa terbaik untuk melakukan prediksi, untuk digunakan nantinya.
"""

scaler = StandardScaler()
scaler.fit(X_train_encoded[:, :3])
X_test_encoded[:, :3] = scaler.transform(X_test_encoded[:, :3])

mse = pd.DataFrame(columns=['train', 'test'],
                   index=['K-Nearest', 'RandomForest', 'AdaBoost'])

model_dict = {'K-Nearest': knn, 'RandomForest': rf, 'AdaBoost': ab}

for name, model in model_dict.items():
  mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train_encoded))/1e3
  mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test_encoded))/1e3

mse

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Dari perbandingan ini, **AdaBoost** memiliki nilai MSE yang lebih kecil dibandingkan model lainnya.

### Tes Prediksi Awal
"""

prediksi = X_test_encoded[:1].copy()
pred_dict = {'y_true':150000}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)

pd.DataFrame(pred_dict)

"""Saat dilakukan 1x percobaan prediksi, K-Neighbors Regressor memiliki nilai prediksi yang paling tinggi. Namun tidak dapat disimpulkan bahwa model tersebut yang terbaik, karena percobaan prediksi baru dilakukan 1x saja.

## Tuning Model

Pada tahap ini, akan dilakukan pencarian kombinasi hyperparameter terbaik yang bisa digunakan untuk masing-masing model.
"""

param_grid_knn = {
    'n_neighbors': [5, 10, 15, 20],
    'weights': ['uniform', 'distance']
}
knn = KNeighborsRegressor()
grid_search_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='neg_mean_squared_error')
grid_search_knn.fit(X_train_encoded, y_train)
print("Best parameters for KNN:", grid_search_knn.best_params_)

param_grid_randomForest = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20],
    'min_samples_split': (5, 10, 15, 20),
    'min_samples_leaf': (2, 5, 10, 15) ,
    'max_features': ['log2', 'sqrt', None],
}

random_forest = RandomForestRegressor()
grid_search_rf = GridSearchCV(random_forest, param_grid_randomForest, cv=5, scoring='neg_mean_squared_error', error_score='raise')
grid_search_rf.fit(X_train_encoded, y_train)
print("Best Parameters for Random Forest:", grid_search_rf.best_params_)

param_grid_boosting = {
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.05, 0.1],
}
boosting = AdaBoostRegressor()
grid_search_boosting = GridSearchCV(boosting, param_grid_boosting, cv=5, scoring='neg_mean_squared_error')
grid_search_boosting.fit(X_train_encoded, y_train)
print("Best Parameters for Boosting:", grid_search_boosting.best_params_)

"""### Re-Modelling Setelah Tuning

Setelah didapatkan hyperparameter terbaik pada tahap awal dari Tuning, saatnya mengimplementasinya ke masing-masing model dengan harapan dapat meningkatkan performanya.
"""

best_models = pd.DataFrame(index=['train_mse', 'test_mse'],
                           columns=['K-Nearest', 'RandomForest', 'AdaBoost'])

best_knn = KNeighborsRegressor(n_neighbors=15,weights='uniform').fit(X_train_encoded, y_train)
best_models.loc['train_mse','K-Nearest'] = mean_squared_error(y_pred = best_knn.predict(X_test_encoded), y_true=y_test)

best_rf = RandomForestRegressor(max_depth=10, max_features='sqrt', min_samples_leaf=2, min_samples_split=20, n_estimators=100).fit(X_train_encoded, y_train)
best_models.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=best_rf.predict(X_test_encoded), y_true=y_test)

best_ab = AdaBoostRegressor(learning_rate=0.01,n_estimators=100).fit(X_train_encoded, y_train)
best_models.loc['train_mse','AdaBoost'] = mean_squared_error(y_pred=best_ab.predict(X_test_encoded), y_true=y_test)

"""### Evaluasi Setelah Tuning

Tahap ini mengukur ulang kinerja masing-masing model setelah diimplementasinya hyperparameter terbaik.
"""

best_model_mse = pd.DataFrame(columns=['train', 'test'], index=['K-Nearest', 'RandomForest', 'AdaBoost'])

best_model_dict = {'K-Nearest': best_knn, 'RandomForest': best_rf, 'AdaBoost': best_ab}

for name, model in best_model_dict.items():
    best_model_mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train_encoded).round())
    best_model_mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test_encoded).round())

best_model_mse

fig, ax = plt.subplots()
best_model_mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Setelah dilakukan Hyperparameter Tuning, **Random Forest** memiliki nilai MSE paling rendah dibandingkan model lainnya. Sehingga disimpulkan bahwa Random Forest adalah **model terbaik** untuk melakukan prediksi nilai ini.

### Tes Prediksi Setelah Tuning
"""

prediksi = X_test_encoded[:1].copy()
pred_dict = {'y_true':150000}
for name, model in best_model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round()

pd.DataFrame(pred_dict)

"""Terakhir dilakukan ulang 1x percobaan prediksi, AdaBoost Regressor memiliki nilai prediksi yang paling tinggi. Namun juga tidak dapat disimpulkan bahwa model tersebut yang terbaik, karena percobaan prediksi baru dilakukan 1x saja."""